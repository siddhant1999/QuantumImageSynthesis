@article{weng2021diffusion,
  title   = "What are diffusion models?",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2021",
  url     = "https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html"
}

@inproceedings{ho2020denoising,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{dhariwal2021diffusion,
      title={Diffusion Models Beat GANs on Image Synthesis}, 
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wu2017learning,
      title={Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling}, 
      author={Jiajun Wu and Chengkai Zhang and Tianfan Xue and William T. Freeman and Joshua B. Tenenbaum},
      year={2017},
      eprint={1610.07584},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{inproceedings,
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
year = {2015},
month = {06},
pages = {1912-1920},
title = {3D ShapeNets: A deep representation for volumetric shapes},
doi = {10.1109/CVPR.2015.7298801}
}

 @misc{nealjean_2018, title={Fréchet inception distance}, url={https://nealjean.com/ml/frechet-inception-distance/}, journal={Neal Jean}, author={Neal Jean}, year={2018}, month={Jul}} 
 
 @article{silico,
  doi = {10.1016/j.cell.2018.03.040},
  url = {https://doi.org/10.1016/j.cell.2018.03.040},
  year = {2018},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {173},
  number = {3},
  pages = {792--803.e19},
  author = {Eric M. Christiansen and Samuel J. Yang and D. Michael Ando and Ashkan Javaherian and Gaia Skibinski and Scott Lipnick and Elliot Mount and Alison O'Neil and Kevan Shah and Alicia K. Lee and Piyush Goyal and William Fedus and Ryan Poplin and Andre Esteva and Marc Berndl and Lee L. Rubin and Philip Nelson and Steven Finkbeiner},
  title = {In Silico Labeling: Predicting Fluorescent Labels in Unlabeled Images},
  journal = {Cell}
}
@BOOK{Jebara2004,
  title     = "Machine Learning",
  author    = "Jebara, Tony",
  publisher = "Springer US",
  year      =  2004,
  address   = "Boston, MA"
}
@ARTICLE{perceptron,
  author={Rosenblatt, Frank},
  journal={Proceedings of the IRE}, 
  title={Perceptron Simulation Experiments}, 
  year={1960},
  volume={48},
  number={3},
  pages={301-309},
  doi={10.1109/JRPROC.1960.287598}}

@incollection{Yadav2015,
  doi = {10.1007/978-94-017-9816-7_2},
  url = {https://doi.org/10.1007/978-94-017-9816-7_2},
  year = {2015},
  publisher = {Springer Netherlands},
  pages = {13--15},
  author = {Neha Yadav and Anupam Yadav and Manoj Kumar},
  title = {History of Neural Networks},
  booktitle = {An Introduction to Neural Network Methods for Differential Equations}
}
@BOOK{Minsky1969,
  title     = "Perceptrons",
  author    = "Minsky, Marvin and Papert, Seymour",
  publisher = "MIT Press",
  month     =  feb,
  year      =  1969,
  address   = "London, England"
}
 @misc{oman_2017, title={Solving XOR with a neural network in Python}, url={https://aimatters.wordpress.com/2016/01/11/solving-xor-with-a-neural-network-in-python/}, journal={On Machine Intelligence}, author={Oman, Stephen}, year={2017}, month={Mar}} 
 @article{ruck1990multilayer,
  title={The multilayer perceptron as an approximation to a Bayes optimal discriminant function},
  author={Ruck, Dennis W and Rogers, Steven K and Kabrisky, Matthew and Oxley, Mark E and Suter, Bruce W},
  journal={IEEE transactions on neural networks},
  volume={1},
  number={4},
  pages={296--298},
  year={1990},
  publisher={IEEE}
}
@article{Rumelhart1986,
  doi = {10.1038/323533a0},
  url = {https://doi.org/10.1038/323533a0},
  year = {1986},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {323},
  number = {6088},
  pages = {533--536},
  author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title = {Learning representations by back-propagating errors},
  journal = {Nature}
}

 @misc{johnson_2022, title={Back Propagation Neural Network: What is backpropagation algorithm in machine learning?}, url={https://www.guru99.com/backpropogation-neural-network.html}, journal={Guru99}, author={Johnson, Daniel}, year={2022}, month={Jan}} 
 
 @inproceedings{precnn,
author = {Denker, J. S. and Gardner, W. R. and Graf, H. P. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D. and Baird, H. S. and Guyon, I.},
title = {Neural Network Recognizer for Hand-Written Zip Code Digits},
year = {1988},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.},
booktitle = {Proceedings of the 1st International Conference on Neural Information Processing Systems},
pages = {323–331},
numpages = {9},
series = {NIPS'88}
}
 
 @ARTICLE{6795724,  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},  journal={Neural Computation},   title={Backpropagation Applied to Handwritten Zip Code Recognition},   year={1989},  volume={1},  number={4},  pages={541-551},  doi={10.1162/neco.1989.1.4.541}}
 
 @ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
  @inproceedings{dan2010,
author = {Ciresan, Dan and Meier, Ueli and Masci, Jonathan and Gambardella, Luca Maria and Schmidhuber, Jürgen},
year = {2011},
month = {07},
pages = {1237-1242},
title = {Flexible, High Performance Convolutional Neural Networks for Image Classification.},
journal = {International Joint Conference on Artificial Intelligence IJCAI-2011},
doi = {10.5591/978-1-57735-516-8/IJCAI11-210}
}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


 @misc{rocca_2021, title={Understanding {V}ariational {A}utoencoders {V}{A}{E}s}, howpublished={https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73}, journal={Medium}, publisher={Towards Data Science}, author={Rocca, Joseph}, year={2021}, month={Mar}} 
 
 @misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
 @InProceedings{rbm,
author="Fischer, Asja
and Igel, Christian",
editor="Alvarez, Luis
and Mejail, Marta
and Gomez, Luis
and Jacobo, Julio",
title="An Introduction to Restricted Boltzmann Machines",
booktitle="Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="14--36",
abstract="Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.",
isbn="978-3-642-33275-3"
}
@article{DBM,
  author    = {Guido Mont{\'{u}}far},
  title     = {Restricted Boltzmann Machines: Introduction and Review},
  journal   = {CoRR},
  volume    = {abs/1806.07066},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.07066},
  eprinttype = {arXiv},
  eprint    = {1806.07066},
  timestamp = {Sat, 23 Jan 2021 01:20:36 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-07066.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{trainingrbms,
author = {Hinton, Geoffrey E.},
title = {Training Products of Experts by Minimizing Contrastive Divergence},
year = {2002},
issue_date = {August 2002},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {14},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/089976602760128018},
doi = {10.1162/089976602760128018},
abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual "expert" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called "contrastive divergence" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
journal = {Neural Comput.},
month = {aug},
pages = {1771–1800},
numpages = {30}
}

@article{Yan2021,
  title = {Enhanced network optimized generative adversarial network for image enhancement},
  author = {Yan, Lingyu and Fu, Jiarun and Wang, Chunzhi and Ye, Zhiwei and Chen, Hongwei and Ling, Hefei},
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {9},
  pages = {14363--14381},
  year = {2021},
  doi = {10.1007/s11042-020-10310-z},
  url = {https://doi.org/10.1007/s11042-020-10310-z},
  abstract = {With the development of image recognition technology, face, body shape, and other factors have been widely used as identification labels, which provide a lot of convenience for our daily life. However, image recognition has much higher requirements for image conditions than traditional identification methods like a password. Therefore, image enhancement plays an important role in the process of image analysis for images with noise, among which the image of low-light is the top priority of our research. In this paper, a low-light image enhancement method based on the enhanced network module optimized Generative Adversarial Networks(GAN) is proposed. The proposed method first applied the enhancement network to input the image into the generator to generate a similar image in the new space, Then constructed a loss function and minimized it to train the discriminator, which is used to compare the image generated by the generator with the real image. We implemented the proposed method on two image datasets (DPED, LOL), and compared it with both the traditional image enhancement method and the deep learning approach. Experiments showed that our proposed network enhanced images have higher PNSR and SSIM, the overall perception of relatively good quality, demonstrating the effectiveness of the method in the aspect of low illumination image enhancement.}
}

@misc{google,
      title={Towards Principled Methods for Training Generative Adversarial Networks}, 
      author={Martin Arjovsky and Léon Bottou},
      year={2017},
      eprint={1701.04862},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
} 

@misc{salimans2016improved,
      title={Improved Techniques for Training GANs}, 
      author={Tim Salimans and Ian Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
      year={2016},
      eprint={1606.03498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{170608500,
Author = {Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
Title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
Year = {2017},
Eprint = {arXiv:1706.08500},
Howpublished = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
}


@article{Jain2020,
  doi = {10.1007/s42452-020-2847-4},
  url = {https://doi.org/10.1007/s42452-020-2847-4},
  year = {2020},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {2},
  number = {6},
  author = {Siddhant Jain and Jalal Ziauddin and Paul Leonchyk and Shashibushan Yenkanchi and Joseph Geraci},
  title = {Quantum and classical machine learning for the classification of non-small-cell lung cancer patients},
  journal = {{SN} Applied Sciences}
}

@article{Thulasidasan2016,
title = {Generative Modeling for Machine Learning on the D-Wave},
author = {Thulasidasan, Sunil},
abstractNote = {These are slides on Generative Modeling for Machine Learning on the D-Wave. The following topics are detailed: generative models; Boltzmann machines: a generative model; restricted Boltzmann machines; learning parameters: RBM training; practical ways to train RBM; D-Wave as a Boltzmann sampler; mapping RBM onto the D-Wave; Chimera restricted RBM; mapping binary RBM to Ising model; experiments; data; D-Wave effective temperature, parameters noise, etc.; experiments: contrastive divergence (CD) 1 step; after 50 steps of CD; after 100 steps of CD; D-Wave (experiments 1, 2, 3); D-Wave observations.},
doi = {10.2172/1332219},
url = {https://www.osti.gov/biblio/1332219}, journal = {},
number = ,
volume = ,
place = {United States},
year = {2016},
month = {11}
}

@article{Amin_2018,
title = {Quantum Boltzmann Machine},
  author = {Amin, Mohammad H. and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
  journal = {Phys. Rev. X},
  volume = {8},
  issue = {2},
  pages = {021050},
  numpages = {11},
  year = {2018},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.8.021050},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.8.021050}
}

@article{xiao2022DDGAN,
  author       = {Zhisheng Xiao and
                  Karsten Kreis and
                  Arash Vahdat},
  title        = {Tackling the Generative Learning Trilemma with Denoising Diffusion
                  GANs},
  journal      = {CoRR},
  volume       = {abs/2112.07804},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.07804},
  eprinttype    = {arXiv},
  eprint       = {2112.07804},
  timestamp    = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-07804.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{ho2021cascaded,
  title={Cascaded Diffusion Models for High Fidelity Image Generation},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal={arXiv preprint arXiv:2106.15282},
  year={2021}
}
@article{Smolensky1986InformationPI,
author = {Smolensky, Paul},
year = {1986},
month = {01},
pages = {},
title = {Information processing in dynamical systems: Foundations of harmony theory},
volume = {1},
journal = {Parallel Distributed Process}
}

@inproceedings{NIPS1991_33e8075e,
 author = {Freund, Yoav and Haussler, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Unsupervised learning of distributions on binary vectors using two layer networks},
 url = {https://proceedings.neurips.cc/paper/1991/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
 volume = {4},
 year = {1991}
}

@article{hopfield,
author = {J J Hopfield },
title = {Neural networks and physical systems with emergent collective computational abilities.},
journal = {Proceedings of the National Academy of Sciences},
volume = {79},
number = {8},
pages = {2554-2558},
year = {1982},
doi = {10.1073/pnas.79.8.2554},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554},
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}}
 
 
 @incollection{Hinton2012,
  doi = {10.1007/978-3-642-35289-8_32},
  url = {https://doi.org/10.1007/978-3-642-35289-8_32},
  year = {2012},
  publisher = {Springer Berlin Heidelberg},
  pages = {599--619},
  author = {Geoffrey E. Hinton},
  title = {A Practical Guide to Training Restricted Boltzmann Machines},
  booktitle = {Lecture Notes in Computer Science}
}

@inproceedings{CarreiraPerpin2005OnCD,
  title={On Contrastive Divergence Learning},
  author={Miguel {\'A}. Carreira-Perpi{\~n}{\'a}n and Geoffrey E. Hinton},
  booktitle={AISTATS},
  year={2005}
}

@article{qubit,
  doi = {10.1038/s42254-021-00410-6},
  url = {https://doi.org/10.1038/s42254-021-00410-6},
  year = {2022},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {4},
  number = {1},
  pages = {1--1},
  title = {40 years of quantum computing},
  journal = {Nature Reviews Physics}
}

@article{Greenspan1982,
  doi = {10.1007/bf02650181},
  url = {https://doi.org/10.1007/bf02650181},
  year = {1982},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {21},
  number = {6-7},
  pages = {505--523},
  author = {Donald Greenspan},
  title = {Deterministic computer physics},
  journal = {International Journal of Theoretical Physics}
}

 @misc{bradben, title={Quantum computing history and background - azure quantum}, url={https://docs.microsoft.com/en-us/azure/quantum/concepts-overview}, journal={Azure Quantum | Microsoft Docs}, author={Bradben}} 
 
 @misc{nobel, title={The Nobel Prize in Physics 1965},
 url={
 https://www.nobelprize.org/prizes/physics/1965/summary/
 }
 journal={NobelPrizeOrg}
 }
 
 @BOOK{Nielsen2000,
  title     = "Cambridge series on information and the natural sciences:
               Quantum computation and quantum information",
  author    = "Nielsen, Michael A and Chuang, Isaac L",
  publisher = "Cambridge University Press",
  month     =  oct,
  year      =  2000,
  address   = "Cambridge, England",
  language  = "en"
}
@article{Shor_1997,
	doi = {10.1137/s0097539795293172},
  
	url = {https://doi.org/10.1137\%2Fs0097539795293172},
  
	year = 1997,
	month = {oct},
  
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  
	volume = {26},
  
	number = {5},
  
	pages = {1484--1509},
  
	author = {Peter W. Shor},
  
	title = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
  
	journal = {{SIAM} Journal on Computing}
}

 @misc{dwavedocs,
	author = {},
	title = {{W}hat is {Q}uantum {A}nnealing? {D}-{W}ave {S}ystem {D}ocumentation},
	howpublished = {\url{https://docs.dwavesys.com/docs/latest/c_gs_2.html}},
	year = {},
}
  @misc{lardinois_2021, title={D-wave plans to build a gate-model quantum computer}, url={https://techcrunch.com/2021/10/05/d-wave-plans-to-build-a-gate-model-quantum-computer/}, journal={TechCrunch}, publisher={TechCrunch}, author={Lardinois, Frederic}, year={2021}, month={Oct}}
  
  
  @article{CIFAR,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
howpublished= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@inproceedings{Krizhevsky09learningmultiple,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  howpublished={https://api.semanticscholar.org/CorpusID:18268744}
}

 @misc{ai_progress_measurement,
 author={ Peter Eckersley, Yomna Nasser et al},
 title={EFF AI Progress Measurement Project},
 year={2017},
 url={https://www.eff.org/ai/metrics}
 }
 
@misc{isscore,
  doi = {10.48550/ARXIV.1606.03498},
  
  url = {https://arxiv.org/abs/1606.03498},
  
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improved Techniques for Training GANs},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 
  @misc{mack_2019, title={A simple explanation of the inception score}, url={https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a}, journal={Medium}, publisher={Octavian}, author={Mack, David}, year={2019}, month={Mar}} 
 

 @misc{inception_model,
  doi = {10.48550/ARXIV.1512.00567},
  
  url = {https://arxiv.org/abs/1512.00567},
  
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rethinking the Inception Architecture for Computer Vision},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{NIPS2017_8a1d6947,
 author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
 url = {https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{fid_gan,
  doi = {10.48550/ARXIV.1706.08500},
  
  url = {https://arxiv.org/abs/1706.08500},
  
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{MMD_KID,
  doi = {10.48550/ARXIV.1801.01401},
  
  url = {https://arxiv.org/abs/1801.01401},
  
  author = {Bińkowski, Mikołaj and Sutherland, Danica J. and Arbel, Michael and Gretton, Arthur},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Demystifying MMD GANs},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{googlecloud, title={Cloud tensor processing units (tpus) | google cloud}, howpublished={https://cloud.google.com/tpu/docs/tpus}, journal={Google}, publisher={Google}} 

@misc{transfer,
  doi = {10.48550/ARXIV.0905.4022},
  
  url = {https://arxiv.org/abs/0905.4022},
  
  author = {Dhillon, Paramveer S. and Foster, Dean and Ungar, Lyle},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Transfer Learning Using Feature Selection},
  
  publisher = {arXiv},
  
  year = {2009},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{dwivedi, title={How is transfer learning done in neural networks and convolutional neural networks?}, url={https://www.analyticssteps.com/blogs/how-transfer-learning-done-neural-networks-and-convolutional-neural-networks}, journal={Analytics Steps}, author={Dwivedi, Rohit}} 

 @misc{sciencehistory , title={Amedeo Avogadro}, url={https://www.sciencehistory.org/historical-profile/amedeo-avogadro}, journal={Science History Institute}, year={2017}, month={Nov}} 
 
  @misc{mathshistory, title={Kinetic theory of gases}, url={https://mathshistory.st-andrews.ac.uk/Projects/Johnson/chapter-6/}, journal={Maths History}} 
 
 @inproceedings{Planck19672O,
  title={2 – On the Theory of the Energy Distribution Law of the Normal Spectrum†},
  author={Max Planck},
  year={1967}
}

@inproceedings{photoe,
  title={Über einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichttspunkt.},
  author={Albert Einstein},
  year={1905}
}
 
 @article{Bohr1913,
  doi = {10.1080/14786441308634955},
  url = {https://doi.org/10.1080/14786441308634955},
  year = {1913},
  month = jul,
  publisher = {Informa {UK} Limited},
  volume = {26},
  number = {151},
  pages = {1--25},
  author = {N. Bohr},
  title = {I. $\less$i$\greater$On the constitution of atoms and molecules$\less$/i$\greater$},
  journal = {The London,  Edinburgh,  and Dublin Philosophical Magazine and Journal of Science}
}

@inproceedings{debroglie,
  title={ Radiations—Ondes et Quanta/Radiation—Waves and Quanta},
  author={L.V. de Broglie},
  publisher={Comptes Rendus Mathématique}
  year={1923}
}

@article{Schrdinger1926,
  doi = {10.1002/andp.19263840404},
  url = {https://doi.org/10.1002/andp.19263840404},
  year = {1926},
  publisher = {Wiley},
  volume = {384},
  number = {4},
  pages = {361--376},
  author = {E. Schr\"{o}dinger},
  title = {Quantisierung als Eigenwertproblem},
  journal = {Annalen der Physik}
}

@article{1928,
  doi = {10.1098/rspa.1928.0023},
  url = {https://doi.org/10.1098/rspa.1928.0023},
  year = {1928},
  month = feb,
  publisher = {The Royal Society},
  volume = {117},
  number = {778},
  pages = {610--624},
  title = {The quantum theory of the electron},
  journal = {Proceedings of the Royal Society of London. Series A,  Containing Papers of a Mathematical and Physical Character}
}

@article{PhysRev,
  title = {The Radiation Theories of Tomonaga, Schwinger, and Feynman},
  author = {Dyson, F. J.},
  journal = {Phys. Rev.},
  volume = {75},
  issue = {3},
  pages = {486--502},
  numpages = {0},
  year = {1949},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.75.486},
  url = {https://link.aps.org/doi/10.1103/PhysRev.75.486}
}

@article{qmlimggen,
  title={A hybrid quantum enabled RBM advantage: convolutional autoencoders for quantum image compression and generative learning},
  author={Jennifer Sleeman and John E. Dorband and Milton Halem},
  booktitle={Defense + Commercial Sensing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211003955}
}

@article{qpugibbs,
 title = {Quantum Enhanced Inference in Markov Logic Networks},
  author = {Wittek, Peter and Gogolin, Christian},
  journal = {Scientific Reports},
 volume = {7},
  number = {1},
  pages = {45672},
  year = {2017},
  doi = {10.1038/srep45672},
  url = {https://doi.org/10.1038/srep45672},
  abstract = {Markov logic networks (MLNs) reconcile two opposing schools in machine learning and artificial intelligence: causal networks, which account for uncertainty extremely well, and first-order logic, which allows for formal deduction. An MLN is essentially a first-order logic template to generate Markov networks. Inference in MLNs is probabilistic and it is often performed by approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling. An MLN has many regular, symmetric structures that can be exploited at both first-order level and in the generated Markov network. We analyze the graph structures that are produced by various lifting methods and investigate the extent to which quantum protocols can be used to speed up Gibbs sampling with state preparation and measurement schemes. We review different such approaches, discuss their advantages, theoretical limitations, and their appeal to implementations. We find that a straightforward application of a recent result yields exponential speedup compared to classical heuristics in approximate probabilistic inference, thereby demonstrating another example where advanced quantum resources can potentially prove useful in machine learning.},
  issn = {2045-2322},
  date = {2017-04-19},
}

@article{quantumCNN,
author = {Wei, Shijie and Chen, Yanhu and Zhou, ZengRong and Long, GuiLu},
year = {2022},
month = {01},
pages = {},
title = {A quantum convolutional neural network on NISQ devices},
volume = {32},
journal = {AAPPS Bulletin},
doi = {10.1007/s43673-021-00030-3}
}
 
 @article{monte,
  doi = {10.1007/s43673-023-00077-4},
  url = {https://doi.org/10.1007/s43673-023-00077-4},
  year = {2023},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {33},
  number = {1},
  author = {Bo Lu and Lu Liu and Jun-Yang Song and Kai Wen and Chuan Wang},
  title = {Recent progress on coherent computation based on quantum squeezing},
  journal = {{AAPPS} Bulletin}
}
 @article{classifiers,
author = {Li, Weikang and Deng, Dong-Ling},
year = {2022},
month = {02},
pages = {220301},
title = {Recent advances for quantum classifiers},
volume = {65},
journal = {Science China Physics, Mechanics & Astronomy},
doi = {10.1007/s11433-021-1793-6}
}
 
 